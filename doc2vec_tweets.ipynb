{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_utils as td\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import iex_utils as ie\n",
    "import numpy as np\n",
    "\n",
    "import trading_strategy as ts\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Util Classes \n",
    "financial_data = ie.IEXData()\n",
    "trump_twitter = td.TwitterApiData()\n",
    "\n",
    "#NUM_EPOCHS = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TENSOR FLOW CODE: \n",
    "\n",
    "\n",
    "#https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
    "def plot_history(histories, key='loss'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    \n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_' + key],\n",
    "                       '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "                 label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0, max(history.epoch)])\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128,  kernel_regularizer=keras.regularizers.l2(0.01), activation=tf.nn.relu),\n",
    "        #keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=None)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def get_compile_settings():\n",
    "    optimizer = 'adam'\n",
    "    loss = 'mean_squared_error'\n",
    "    metrics = ['mean_squared_error']\n",
    "    return optimizer, loss, metrics\n",
    "\n",
    "\n",
    "def train_model(train_obs, train_labels, model, num_epochs, validation_obs, validation_labels):\n",
    "    my_optimizer, my_loss, my_metrics = get_compile_settings()\n",
    "    model.compile(optimizer=my_optimizer, loss=my_loss, metrics=my_metrics)\n",
    "    base_model_history = model.fit(train_obs, train_labels, epochs=num_epochs, validation_data = (validation_obs, validation_labels))\n",
    "    plot_history([('base_model',base_model_history)])\n",
    "    return model\n",
    "\n",
    "def test_model(test_obs, test_labels, model):\n",
    "    test_loss, test_acc = model.evaluate(test_obs, test_labels)\n",
    "    print(\"Test loss:\", test_acc)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_data(composite_dict, d2v_model):\n",
    "    X = []\n",
    "    Y = []\n",
    "    #date_to_embeddings = {}\n",
    "    date_list = []\n",
    "    for date, v in composite_dict.items():\n",
    "        price = v['price']\n",
    "        \n",
    "        embedding_tweets = []\n",
    "        documents = v['tweets']\n",
    "        for tweet_text in documents:\n",
    "            d2v_model.random.seed(0)\n",
    "            embedding_tweets.append(d2v_model.infer_vector(word_tokenize(tweet_text.lower())))\n",
    "            \n",
    "        embedding_average = np.mean(embedding_tweets, axis=0)\n",
    "        #date_to_embeddings[date] = embedding_average\n",
    "        \n",
    "        X.append(embedding_average)\n",
    "        Y.append(price)\n",
    "        date_list.append(date)\n",
    "\n",
    "    return np.array(X), np.array(Y), date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_and_stock_data(tweets, stock_data):\n",
    "    date_to_tweet = {}\n",
    "    date_to_price = {}\n",
    "\n",
    "    for t in tweets:\n",
    "        date = t[0]\n",
    "        if date not in date_to_tweet:\n",
    "            date_to_tweet[date] = []\n",
    "        date_to_tweet[date].append(t[1])\n",
    "    for dp in stock_data:\n",
    "        date = dp[0]\n",
    "        date_to_price[date] = dp[1]\n",
    "        \n",
    "    composite_dict = {}\n",
    "    lag = 0\n",
    "    '''\n",
    "    for date, price in date_to_price.items():\n",
    "        print(date)\n",
    "        if date in date_to_tweet and len(date_to_tweet[date]) > 0:\n",
    "            composite_dict[date] = {'price': price, 'tweets':date_to_tweet[date]}\n",
    "    '''\n",
    "    \n",
    "\n",
    "    for date, tweets in date_to_tweet.items():\n",
    "        date_adjusted = date + timedelta(days=lag)\n",
    "        if date_adjusted not in date_to_price:\n",
    "            continue\n",
    "        composite_dict[date_adjusted] = {'price': date_to_price[date_adjusted], 'tweets': tweets}\n",
    "        \n",
    "    return composite_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates_for_posts(posts):\n",
    "    result = []\n",
    "    for p in posts: \n",
    "        date = p[0]\n",
    "        new_date = (date.year, date.month, date.day)\n",
    "        p = list(p)\n",
    "        p[0] = new_date\n",
    "        p = tuple(p)\n",
    "        result.append(p)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc2vec_model(data):\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "    max_epochs = 100\n",
    "    vec_size = 500\n",
    "    alpha = 0.025\n",
    "\n",
    "    model = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(\"sample.model\")\n",
    "    print(\"Model Saved\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_posts(first_year, last_year):\n",
    "    years = range(first_year, last_year + 1, 1)\n",
    "    posts = []\n",
    "\n",
    "    for year in years: \n",
    "        posts += trump_twitter.get_posts(year, [])\n",
    "    #posts = format_dates_for_posts(posts)\n",
    "    return posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(twitter_posts, train_start_date, train_end_date, validation_start_date, validation_end_date, test_start_date, test_end_date, company_ticker, num_epochs):\n",
    "    #d2v_model = build_doc2vec_model([p[1] for p in posts])\n",
    "    d2v_model = Doc2Vec.load('sample.model')\n",
    "\n",
    "    stock_data_train = financial_data.get_stock_price_for_ticker(company_ticker, train_start_date, train_end_date)\n",
    "    #print(len([d[1] for d in stock_data_train if d[1] > 0]))\n",
    "    #print(len([d[1] for d in stock_data_train if d[1] < 0]))\n",
    "\n",
    "    composite_dict_train = get_tweet_and_stock_data(posts, stock_data_train)\n",
    "    train_obs, train_labels, _ = get_xy_data(composite_dict_train, d2v_model)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(train_obs)\n",
    "    train_obs = scaler.transform(train_obs)\n",
    "    \n",
    "    stock_data_validation = financial_data.get_stock_price_for_ticker(company_ticker, validation_start_date, validation_end_date)\n",
    "    composite_dict_validation = get_tweet_and_stock_data(posts, stock_data_validation)\n",
    "    validation_obs, validation_labels, _ = get_xy_data(composite_dict_validation, d2v_model)\n",
    "    validation_obs = scaler.transform(validation_obs)\n",
    "    \n",
    "    stock_data_test = financial_data.get_stock_price_for_ticker(company_ticker, test_start_date, test_end_date)\n",
    "    composite_dict_test = get_tweet_and_stock_data(posts, stock_data_test)\n",
    "    test_obs, test_labels, date_list_test = get_xy_data(composite_dict_test, d2v_model)\n",
    "    test_obs = scaler.transform(test_obs)\n",
    "    \n",
    "    print(len(date_list_test) == len(test_obs))\n",
    "    \n",
    "    date_to_embeddings_test = {}\n",
    "    for i in range(len(test_obs)):\n",
    "        date_to_embeddings_test[date_list_test[i]] = test_obs[i]\n",
    "    \n",
    "    model = build_model()\n",
    "    model = train_model(train_obs, train_labels, model, num_epochs, validation_obs, validation_labels)\n",
    "    model_acc = test_model(test_obs, test_labels, model)\n",
    "    \n",
    "    predictions = model.predict(test_obs)\n",
    "    print(predictions)\n",
    "    print('SIZE OF TRAIN SET: ', len(train_labels))\n",
    "    print('SIZE OF TEST SET: ', len(test_labels))\n",
    "    print('SIZE OF VALIDATION SET: ', len(validation_labels))\n",
    "    \n",
    "    print('TOTAL SIZE: ', len(train_labels) + len(test_labels) + len(validation_labels))\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(len(predictions)):\n",
    "        p = predictions[i]\n",
    "        t = test_labels[i]\n",
    "        if (p >= 0) and (t >= 0):\n",
    "            counter += 1\n",
    "        elif (p < 0) and (t < 0):\n",
    "            counter += 1\n",
    "    \n",
    "    return counter, date_to_embeddings_test, model, test_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date = datetime(2010, 1, 1)\n",
    "train_end_date = datetime(2015, 5, 30)\n",
    "\n",
    "\n",
    "validation_start_date = datetime(2015, 5, 31)\n",
    "validation_end_date = datetime(2016, 10, 31)\n",
    "\n",
    "test_start_date = datetime(2016, 11, 1)\n",
    "test_end_date = datetime(2019, 6, 10)\n",
    "\n",
    "posts = get_all_posts(2010, 2019)\n",
    "\n",
    "\n",
    "\n",
    "company_ticker = 'IWV'\n",
    "num_epochs = 100\n",
    "\n",
    "error, date_to_tweets_test, nn_model, test_obs = run_simulation(posts, train_start_date, train_end_date,validation_start_date, validation_end_date, test_start_date, test_end_date, company_ticker, num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "ts.run_strategy(nn_model, company_ticker, results_dict, test_start_date, date_to_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for k, v in results_dict.items(): \n",
    "    X.append(k)\n",
    "    Y.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y)\n",
    "plt.title('Simulated Trading Based on Trump Tweet Predictions')\n",
    "plt.ylabel('Portfolio Value')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "results_array = []\n",
    "for num_epochs in range(100, 500, 100):\n",
    "    print('=====================')\n",
    "    print('Testing with Epoch value: ', num_epochs)\n",
    "    for j in range(5):\n",
    "        error = run_simulation(posts, train_start_date, train_end_date, test_start_date, test_end_date, company_ticker, num_epochs)\n",
    "        results_array.append((j, error))\n",
    "    for result in results_array:\n",
    "        print('Trial Number: ', result[0], ' Num Correct: ', result[1])\n",
    "    print('=====================')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
